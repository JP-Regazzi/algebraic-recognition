{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data, Batch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from generate_dataset import generate_dataset\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_DATASET_SIZE = 130\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_geometric_representation(in_graph_dict: dict, encoder) -> Data:\n",
    "    node_list = []\n",
    "    values = []\n",
    "    edge_mappings = []\n",
    "    def traverse_graph(graph = in_graph_dict):\n",
    "        nonlocal node_list\n",
    "        nonlocal edge_mappings\n",
    "        nonlocal values\n",
    "        node_list.append(graph[\"id\"])\n",
    "        values.append(str(graph[\"val\"]))\n",
    "        if hasattr(graph,\"children\"):\n",
    "            for child in graph[\"children\"]:\n",
    "                edge_mappings.append((graph[\"id\"], child[\"id\"]) )\n",
    "                traverse_graph(child)\n",
    "    traverse_graph()\n",
    "    nodes = torch.tensor(node_list,dtype=torch.long)\n",
    "    edges = torch.tensor([[x[0] for x in edge_mappings], [x[1] for x in edge_mappings]], dtype=torch.long) # Probably slow and mentally degenerated\n",
    "    geom_data = Data(nodes, edges)\n",
    "    return geom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPERATIONS = [\"ADD\", \"MUL\", \"FUNC\", \"POW\"]\n",
    "FUNCTIONS = [\"SIN\", \"COS\", \"TAN\", \"EXP\", \"LOG\", \"f\", \"g\", \"h\"]\n",
    "ATOMICS = [\"LITERAL\", \"VARIABLE\"]\n",
    "VARIABLE_ALPHABET = [chr(x) for x in range(ord(\"a\"), ord(\"z\")+1) if chr(x) not in [\"f\", \"g\", \"h\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_node_attribute_encoder(label_encoder:LabelEncoder, rep = 3):\n",
    "    def node_attr_encoder(attr):\n",
    "        if isinstance(attr, str):\n",
    "            res = label_encoder.transform(attr)\n",
    "            return [res]*(rep + 1)\n",
    "        else:\n",
    "            return [0] + [attr]*rep\n",
    "            \n",
    "    return node_attr_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MathExpressionDataset(InMemoryDataset):\n",
    "    def __init__(self, root, expression, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.expr = expression\n",
    "        self.load(self.processed_paths[0])\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(OPERATIONS+FUNCTIONS+ATOMICS+VARIABLE_ALPHABET)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['math_datagen.json']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = []\n",
    "        for file in self.raw_file_names:\n",
    "            with open(file) as file_handle:\n",
    "                object_data = json.load(file_handle)\n",
    "                for comparison in object_data:\n",
    "                    expr = comparison[self.expr]\n",
    "                    score = comparison[\"score\"]\n",
    "                    geometric_expr = dict_to_geometric_representation(expr, make_node_attribute_encoder(self.le))\n",
    "                    geometric_expr.y = score\n",
    "                    data_list.append(geometric_expr)\n",
    "                    \n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionPairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__()\n",
    "        self.dataset_l = MathExpressionDataset(root,\"expr_l\",transform=None, pre_transform=None, pre_filter=None)\n",
    "        self.dataset_r = MathExpressionDataset(root,\"expr_r\",transform=None, pre_transform=None, pre_filter=None)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_l[idx], self.dataset_r[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset(130,\"math_datagen.json\")\n",
    "dataset = ExpressionPairDataset(root=\"/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormulaNet(nn.Module):\n",
    "    def __init__(self, hidden_channels, embedding_space):\n",
    "        super(FormulaNet, self).__init__()\n",
    "        self.dense_1 = Linear(dataset.num_features, 16) \n",
    "        self.relu_1 = ReLU()\n",
    "        self.gconv_1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.gconv_2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.dense_2 = Linear(hidden_channels, embedding_space)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseFormulaNet(nn.Module):\n",
    "    def __init__(self, hidden_channels, embedding_space):\n",
    "        super(SiameseFormulaNet, self).__init__()\n",
    "        self.formulanet_1 = FormulaNet(hidden_channels, embedding_space)\n",
    "        self.formulanet_2 = FormulaNet(hidden_channels, embedding_space)\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        pass\n",
    "\n",
    "    def forward(self, expr_l, expr_r):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(dataset, list(range(TRAIN_SAMPLES)))\n",
    "test_dataset = Subset(dataset, list(range(TRAIN_SAMPLES, len(dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate(self, data_list):\n",
    "    batchA = Batch.from_data_list([data[0] for data in data_list])\n",
    "    batchB = Batch.from_data_list([data[1] for data in data_list])\n",
    "    return batchA, batchB\n",
    "# NOTE: Type ignore only for collate_fn_t ... make sure it doesn't get in the way of correct typing for the dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate) # type: ignore\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate) # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
