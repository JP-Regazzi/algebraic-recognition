{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data, Batch\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear, ReLU, LeakyReLU, init\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import numpy as np\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILEPATH = \"math_datagen_triplet_8k.json\"\n",
    "BATCH_SIZE = 256\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = [\"FUNC\", \"OPERATION\", \"POW\", \"LITERAL\", \"VARIABLE\", \"CONSTANT_LITERAL\"]\n",
    "OPERATIONS = [\"ADD\", \"MUL\", \"FUNC\", \"POW\"]\n",
    "FUNCTIONS = [\"SIN\", \"COS\", \"TAN\", \"EXP\", \"LOG\", \"SINH\", \"COSH\", \"TANH\"]\n",
    "VARIABLE_ALPHABET = [chr(x) for x in range(ord('a'), ord('z')+1) if chr(x) not in [\"e\", \"i\"]]\n",
    "CLASSIC_CONSTANTS = [\"PI\", \"I\", \"g\",\"e\", \"zoo\", \"E\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dict_to_geometric_representation(in_graph_dict: dict, encoder) -> Data:\n",
    "    node_list = []\n",
    "    edge_mappings = []\n",
    "    def traverse_graph(graph = in_graph_dict):\n",
    "        nonlocal node_list\n",
    "        nonlocal edge_mappings\n",
    "        curr_node_index = len(node_list)\n",
    "        encoded_data = encoder({\"type\": graph[\"type\"], \"subtype\": graph[\"subtype\"], \"value\": graph[\"value\"]})\n",
    "        # print(\"Encoded data: \", encoded_data)\n",
    "        node_list.append(encoded_data)\n",
    "        if \"children\" in graph.keys():\n",
    "            for child in graph[\"children\"]:\n",
    "                edge_mappings.append((curr_node_index, traverse_graph(child)) ) #I'm retarded.\n",
    "        return curr_node_index\n",
    "    traverse_graph()\n",
    "    nodes = torch.tensor(node_list,dtype=torch.float32)\n",
    "    if not torch.all(torch.isfinite(nodes)):\n",
    "        print(\"NODES CONTAIN INFINITE VALUE ---> \\n\")\n",
    "        print(\"X = \", nodes)\n",
    "    nodes[nodes == float(\"inf\")] = 0\n",
    "    nodes[nodes == -float(\"inf\")] = 0\n",
    "    if not torch.all(torch.isfinite(nodes)):\n",
    "        print(\"MODIFIED NODES CONTAIN INFINITE VALUE ---> \\n\")\n",
    "        print(\"X = \", nodes)\n",
    "    edges = torch.tensor([[x[0] for x in edge_mappings], [x[1] for x in edge_mappings]], dtype=torch.long) # Probably slow and mentally degenerated\n",
    "    geom_data = Data(x=nodes, edge_index=edges)\n",
    "    return geom_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(OPERATIONS+FUNCTIONS+CLASSIC_CONSTANTS+VARIABLE_ALPHABET+TYPES)\n",
    "# --- misc\n",
    "nan_count = 0 \n",
    "sample_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_value(value):\n",
    "    if value in [\"-Infinity\",\"Infinity\", float(\"inf\"), float(\"-inf\"), \"inf\", \"-inf\"]:\n",
    "        return None\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_attr_encoder(attr):\n",
    "    global nan_count\n",
    "    global sample_count\n",
    "    \n",
    "    def safe_label_encode(value):\n",
    "        return label_encoder.transform([value])[0] if value else -1\n",
    "\n",
    "    type_encoding = [safe_label_encode(attr.get(\"type\"))] * rep\n",
    "    subtype_encoding = [safe_label_encode(attr.get(\"subtype\"))] * rep\n",
    "    \n",
    "    value_encoding_vec = [-1] * rep  # Default to all -1 if no valid value\n",
    "    \n",
    "    if sanitize_value(attr.get(\"value\")) is not None:\n",
    "        if isinstance(attr[\"value\"], str):\n",
    "            value_encoding = safe_label_encode(attr[\"value\"])\n",
    "            value_encoding_vec = [(0 if i % 2 == 0 else value_encoding) for i in range(rep)]\n",
    "        else:\n",
    "            try:\n",
    "                val = float(attr[\"value\"])\n",
    "                if math.isfinite(val):\n",
    "                    expon = math.log10(abs(val))\n",
    "                    val = val // 10**int(expon)\n",
    "                    value_encoding_vec = [(expon if i % 2 == 1 else val) for i in range(rep)]\n",
    "                else:\n",
    "                    raise ValueError(\"Non-finite value\")\n",
    "            except (ValueError, TypeError):\n",
    "                nan_count += 1\n",
    "                value_encoding_vec = [(-1 if i % 2 == 1 else 1) for i in range(rep)]\n",
    "            finally:\n",
    "                sample_count += 1\n",
    "\n",
    "    result = type_encoding + subtype_encoding + value_encoding_vec\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\"value\": Infinity,'.find(\"Infinity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_FILEPATH) as file_handle:\n",
    "    object_data = json.load(file_handle)\n",
    "    for triplet in object_data:\n",
    "        ## TODO : check if contains infinity (triplet to str and then search for Infinity)\n",
    "        # print(triplet)\n",
    "        # print(json.dumps(triplet).find(\"inf\"))\n",
    "        \n",
    "        if json.dumps(triplet).find(\"inf\") == -1:\n",
    "            expr_true = triplet[\"expr_true\"]\n",
    "            expr_false = triplet[\"expr_false\"]\n",
    "            expr_anchor = triplet[\"expr_anchor\"]\n",
    "            geom_anchor = dict_to_geometric_representation(expr_anchor,node_attr_encoder )\n",
    "            geom_true = dict_to_geometric_representation(expr_true,node_attr_encoder )\n",
    "            geom_false = dict_to_geometric_representation(expr_false,node_attr_encoder )\n",
    "            triplet_dataset.append((geom_anchor, geom_true, geom_false))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"test\",\"w\") as f:\n",
    "#     f.write(str(triplet_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = triplet_dataset[0][0].num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormulaNet(nn.Module):\n",
    "    def __init__(self, hidden_channels: int, embedding_space: int):\n",
    "        super(FormulaNet, self).__init__()\n",
    "        self.dense_1 = Linear(num_features, num_features*2) \n",
    "        self.dense_2 = Linear(num_features*2, num_features*2) \n",
    "        \n",
    "        self.relu_1 = LeakyReLU(0.2)\n",
    "        self.gconv_1 = GCNConv(num_features*2, hidden_channels)\n",
    "        self.gconv_2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.dense_3 = Linear(hidden_channels, embedding_space)\n",
    "        gain = torch.nn.init.calculate_gain(\"leaky_relu\", 0.2)\n",
    "        # Initialize dense_1 weights\n",
    "        init.xavier_uniform_(self.dense_1.weight, gain=gain)\n",
    "        init.xavier_uniform_(self.dense_2.weight, gain=gain)\n",
    "        # # Initialize gconv_1 weights\n",
    "        init.xavier_uniform_(self.gconv_1.lin.weight.data.T, gain=gain)\n",
    "        # # Initialize gconv_2 weights\n",
    "        init.xavier_uniform_(self.gconv_2.lin.weight.data.T, gain=gain)\n",
    "        #Initialize dense_3 weights\n",
    "        init.xavier_uniform_(self.dense_3.weight, gain=gain)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        if not torch.all(torch.isfinite(x)):\n",
    "            print(\"INPUT CONTAINS INFINITE VALUE\")\n",
    "            #print(\"x = \", x)\n",
    "            \n",
    "\n",
    "\n",
    "        nan_mask = torch.isnan(x)\n",
    "        if nan_mask.any():\n",
    "            print(\"Some values of the input are nan: \", x[nan_mask])\n",
    "            # Replace NaNs with 0s\n",
    "            x[nan_mask] = -11\n",
    "            \n",
    "        \n",
    "        # print(\"X = \", str(x))\n",
    "        x = self.dense_1(x)\n",
    "        # print(\"d1 X = \", str(x))\n",
    "        x = self.relu_1(x)\n",
    "        \n",
    "        x = self.dense_2(x)\n",
    "        x = self.relu_1(x)\n",
    "\n",
    "        # #print(\"rd1 X = \", str(x))\n",
    "        x = self.gconv_1(x, edge_index)\n",
    "        # print(\"gcn1 X = \", str(x))\n",
    "        x = self.relu_1(x)\n",
    "        # print(\"rgcn1 X = \", str(x))\n",
    "        # x = self.gconv_2(x, edge_index)\n",
    "        # # print(\"gcn2 X = \", str(x))\n",
    "        # x = self.relu_1(x)\n",
    "        # # print(\"rgcn2 X = \", str(x))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.4,training=self.training)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.relu_1(x)\n",
    "        # nan_mask = torch.isnan(x)\n",
    "        # if nan_mask.any():\n",
    "        #     print(\"Some values of the input are nan: \", x[nan_mask])\n",
    "            \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletFormulaNet(nn.Module):\n",
    "    def __init__(self, hidden_channels, embedding_space):\n",
    "        super(TripletFormulaNet, self).__init__()\n",
    "        self.formulanet = FormulaNet(hidden_channels, embedding_space)\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Compute the embeddings for the anchor, positive, and negative inputs\n",
    "        embed_anchor = self.formulanet(anchor.x, anchor.edge_index, anchor.batch)\n",
    "        embed_positive = self.formulanet(positive.x, positive.edge_index, positive.batch)\n",
    "        embed_negative = self.formulanet(negative.x, negative.edge_index, negative.batch)\n",
    "        \n",
    "        return embed_anchor, embed_positive, embed_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK DATASET:\n",
    "with open(\"test\", \"w\") as f:\n",
    "    for a, b,c in triplet_dataset:\n",
    "        f.write(str(a.x) + \"\\n\" + str(b.x) + \"\\n\" + str(c.x) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    \n",
    "    Args:\n",
    "        anchor (torch.Tensor): Embedding of the anchor data point.\n",
    "        positive (torch.Tensor): Embedding of the positive data point (same class as anchor).\n",
    "        negative (torch.Tensor): Embedding of the negative data point (different class from anchor).\n",
    "        margin (float, optional): Margin value for triplet loss. Defaults to 1.0.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Triplet loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the Euclidean distances between the embeddings\n",
    "    distance_positive = F.pairwise_distance(anchor, positive)\n",
    "    distance_negative = F.pairwise_distance(anchor, negative)\n",
    "    \n",
    "    # Calculate the triplet loss\n",
    "    losses = F.relu(distance_positive - distance_negative + margin)\n",
    "    \n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = len(triplet_dataset)\n",
    "train_len = int(TRAIN_RATIO * total_len)\n",
    "val_len = int(VAL_RATIO * total_len)\n",
    "test_len = total_len - train_len - val_len\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(triplet_dataset, [train_len, val_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data_list):\n",
    "    batchA = Batch.from_data_list([data[0] for data in data_list])\n",
    "    batchB = Batch.from_data_list([data[1] for data in data_list])\n",
    "    batchC = Batch.from_data_list([data[2] for data in data_list])\n",
    "    return batchA, batchB, batchC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle= False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TripletFormulaNet(32,32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=0.0005, total_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = []  # Store training loss per epoch\n",
    "validation_loss = []  # Store validation loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_anchor, batch_positive, batch_negative in tqdm(train_loader, desc=f'Epoch {epoch}'):\n",
    "\n",
    "        batch_anchor, batch_positive, batch_negative = batch_anchor.to(device), batch_positive.to(device), batch_negative.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embed_anchor, embed_positive, embed_negative = model(batch_anchor, batch_positive, batch_negative)\n",
    "        loss = triplet_loss(embed_anchor, embed_positive, embed_negative, margin=1.0)\n",
    "        loss.backward()\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        \n",
    "        # Log gradients\n",
    "        # for name, param in model.formulanet.named_parameters():\n",
    "        #     if param.grad is not None and param.grad.numel() > 0:\n",
    "        #         writer.add_histogram(f'{name}.grad', param.grad, epoch)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    scheduler.step(epoch)\n",
    "    training_loss.append(epoch_loss / len(train_loader))  # Store epoch average loss\n",
    "    print(f'Epoch {epoch}, Loss: {epoch_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_loader, device):\n",
    "    model.eval()\n",
    "    triplet_loss_tot = 0\n",
    "    correct_triplets = 0\n",
    "    total_triplets = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_anchor, batch_positive, batch_negative in validation_loader:\n",
    "            batch_anchor, batch_positive, batch_negative = batch_anchor.to(device), batch_positive.to(device), batch_negative.to(device)\n",
    "            embed_anchor, embed_positive, embed_negative = model(batch_anchor, batch_positive, batch_negative)\n",
    "\n",
    "            # Compute triplet loss\n",
    "            loss = triplet_loss(embed_anchor, embed_positive, embed_negative, margin=1.0)\n",
    "            triplet_loss_tot += loss.item() * batch_anchor.size(0)\n",
    "\n",
    "            # Compute triplet accuracy\n",
    "            distance_positive = F.pairwise_distance(embed_anchor, embed_positive)\n",
    "            distance_negative = F.pairwise_distance(embed_anchor, embed_negative)\n",
    "            correct_triplets += (distance_positive < distance_negative).sum().item()\n",
    "            total_triplets += distance_positive.size(0)\n",
    "\n",
    "    triplet_loss_tot /= len(validation_loader.dataset)\n",
    "    triplet_accuracy = correct_triplets / total_triplets\n",
    "\n",
    "    print(f'Validation Triplet Loss: {triplet_loss_tot}, Triplet Accuracy: {triplet_accuracy}')\n",
    "\n",
    "    return triplet_loss_tot, triplet_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 150/150 [00:07<00:00, 21.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.936689825057983\n",
      "Validation Triplet Loss: 37.549303402230144, Triplet Accuracy: 0.226484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 150/150 [00:06<00:00, 21.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.770085296630859\n",
      "Validation Triplet Loss: 36.80406214598566, Triplet Accuracy: 0.22765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 150/150 [00:06<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 5.627102230389913\n",
      "Validation Triplet Loss: 36.10427328631282, Triplet Accuracy: 0.22671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 150/150 [00:07<00:00, 21.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 5.555323699315389\n",
      "Validation Triplet Loss: 35.43232946626842, Triplet Accuracy: 0.22671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 150/150 [00:07<00:00, 19.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 5.438143167495728\n",
      "Validation Triplet Loss: 34.83094189401716, Triplet Accuracy: 0.226953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 150/150 [00:06<00:00, 22.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 5.371730017662048\n",
      "Validation Triplet Loss: 34.27489921368659, Triplet Accuracy: 0.226796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 150/150 [00:07<00:00, 19.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 5.27255446434021\n",
      "Validation Triplet Loss: 33.741417535580695, Triplet Accuracy: 0.2278125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 150/150 [00:08<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 5.21879915078481\n",
      "Validation Triplet Loss: 33.24720354553312, Triplet Accuracy: 0.2271875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 150/150 [00:08<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 5.14193672657013\n",
      "Validation Triplet Loss: 32.797125057838855, Triplet Accuracy: 0.22734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 150/150 [00:07<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 5.051755712827046\n",
      "Validation Triplet Loss: 32.38576854676008, Triplet Accuracy: 0.227421875\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    validate(model, validation_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_left = {\n",
    "      \"type\": \"POW\",\n",
    "      \"value\": None,\n",
    "      \"subtype\": None,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"type\": \"POW\",\n",
    "          \"value\": None,\n",
    "          \"subtype\": None,\n",
    "          \"children\": [\n",
    "            {\n",
    "              \"type\": \"LITERAL\",\n",
    "              \"value\": -3.5,\n",
    "              \"subtype\": None,\n",
    "              \"children\": []\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"LITERAL\",\n",
    "              \"value\": 3.0,\n",
    "              \"subtype\": None,\n",
    "              \"children\": []\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"LITERAL\",\n",
    "          \"value\": 4.5,\n",
    "          \"subtype\": None,\n",
    "          \"children\": []\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "expr_right = {\n",
    "      \"type\": \"POW\",\n",
    "      \"value\": None,\n",
    "      \"subtype\": None,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"type\": \"OPERATION\",\n",
    "          \"value\": None,\n",
    "          \"subtype\": \"ADD\",\n",
    "          \"children\": [\n",
    "            {\n",
    "              \"type\": \"LITERAL\",\n",
    "              \"value\": -5.5,\n",
    "              \"subtype\": None,\n",
    "              \"children\": []\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"LITERAL\",\n",
    "              \"value\": -10.0,\n",
    "              \"subtype\": None,\n",
    "              \"children\": []\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"LITERAL\",\n",
    "          \"value\": 4.0,\n",
    "          \"subtype\": None,\n",
    "          \"children\": []\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "expr_right_constant_error ={\n",
    "      \"type\": \"POW\",\n",
    "      \"value\": None,\n",
    "      \"subtype\": None,\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"type\": \"OPERATION\",\n",
    "          \"value\": None,\n",
    "          \"subtype\": \"ADD\",\n",
    "          \"children\": [\n",
    "            {\n",
    "              \"type\": \"LITERAL\",\n",
    "              \"value\": -5.5,\n",
    "              \"subtype\": None,\n",
    "              \"children\": []\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"LITERAL\",\n",
    "              \"value\": -10.0,\n",
    "              \"subtype\": None,\n",
    "              \"children\": []\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"LITERAL\",\n",
    "          \"value\": 4.0,\n",
    "          \"subtype\": None,\n",
    "          \"children\": []\n",
    "        }\n",
    "      ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_l_obj = dict_to_geometric_representation(expr_left, node_attr_encoder)\n",
    "expr_r_obj = dict_to_geometric_representation(expr_right, node_attr_encoder)\n",
    "expr_r_c_obj = dict_to_geometric_representation(expr_right_constant_error, node_attr_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_l, emb_r, emb_r_c = model(expr_l_obj, expr_r_obj, expr_r_c_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0355], grad_fn=<NormBackward1>)\n",
      "tensor([1.9440], grad_fn=<NormBackward1>)\n",
      "tensor([1.9969], grad_fn=<NormBackward1>)\n",
      "tensor([5.6569e-06], grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(F.pairwise_distance(emb_l, emb_r))\n",
    "print(F.pairwise_distance(emb_l, emb_r_c))\n",
    "print(F.pairwise_distance(emb_r, emb_r_c))\n",
    "print(F.pairwise_distance(emb_r, emb_r))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
